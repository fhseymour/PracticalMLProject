---
title: "Practical ML Class Project"
author: "Fred H. Seymour"
date: "04/20/2015"
output: html_document
---

# Analysis of Weight Lifting Dataset

### Overview

A machine learning algorithm was built to predict weight lifting quality from activity motion monitoring data.  A random forest approach was used with a 70/30 cross-validation split on the training dataset.  Analysis was confined to the 36 numeric feature variables tied to (x,y,z).  The random forest out-of-bag (OOB) error was estimated to be 1.4% and the cross validation out-of-sample error was 1.2%.

This data indicates that these types of sensors could be used to provide real time feedback to an individual on the quality of their weight lifting exercises. 

### Data Preparation

The training dataset was downloaded from : https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

It consists of 19,622 observations of the Unilateral Dumbbell Bicepts Curl weight lift performed by  volunteers who did them correctly and incorrectly with measurement equipment attached.  A detailed description can be found in the study publication at :  http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf   

The dataset has 160 columns (1 y-result and 159 X-features).
   
The y-result (classe) consists of 5 possible outcomes:  
A = lift performed correctly   
B = throwing the elbows to the front   
C = lifting the dumbbell only halfway   
D = lowering the dumbbell only halfway   
E = throwing the hips to the front  
   
The R libraries and code to load the data are shown below.  An outlier magnet dumbbell datapoint is set to its class mean.   

```{r}
library(caret)
library(randomForest)
data.dir <- "/home/fred/LargeDatasets/JohnsHopkinsCoursera/"
train.file <- paste0(data.dir,"pml-training.csv")
maxrow=-1
raw.train <- read.csv(train.file, stringsAsFactors=TRUE, nrows=maxrow)
# clean up an outlier in raw.train$magnet_dumbbell_y with a high negative value
raw.train$magnet_dumbbell_y[which(raw.train$magnet_dumbbell_y == 
                                   min(raw.train$magnet_dumbbell_y))] <-
    mean(raw.train$magnet_dumbbell_y[which(raw.train$classe=="B" &
                                          raw.train$magnet_dumbbell_y < 0)])
```

This raw training dataset is split into a training dataset (70%) and a cross validation test dataset (30%).   
   
For missing values (NAs), the features either had no missing data or 13,435 missing observations (98%).  Based on this, any features with missing values were eliminated reducing the feature count from 159 to 92.  To  simplify the analysis, the features were further downselected to the 36 that include the (x,y,z) axes.    These consist of the combinations of three measuring devices (accelerometer, gyroscope, magnetometer), four attachement locations (belt, arm, forearm, dumbbell) and measurements along the three axes (x,y,z). Based on the motion physics of lifting dumbbells, these features should be sufficient to predict the desired quality outcomes.    

```{r}
# Create cross-validation partition
set.seed(333)
trainIndex = createDataPartition(raw.train$classe, p=0.70,list=FALSE)
training <- raw.train[trainIndex,]
testing <- raw.train[-trainIndex,]
# Downselect training set columns to those that have no NAs
# and that are "_x", "_y", "_z" coordinate based
train.nacount <- apply(training,2,function(x) { sum(is.na(x)) })
unique(train.nacount)
# Only select features where there are no NAs
training1 <- training[,train.nacount==0] 
# select x,y,z features
cn <- names(training1)
training2 <- training1[,c(grep("_x",cn),grep("_y",cn),grep("_z",cn))]
cn <- names(training2)
training3 <- training2[,-c(grep("yaw",cn))]
# re-assemble with outcomes 
training4 <- cbind(classe=training$classe, training3)
```

The training set outcomes are sufficiently balanced that a random forest machine learning approach can be used.

```{r}
table(training4$classe)
```

### Build ML model and show estimated out-of_sample error

The default R random forest parameters are used.   

```{r}
start.time <- Sys.time()
modFit <- train(classe~., data=training4, method="rf")
stop.time <- Sys.time()
stop.time - start.time
fm <- modFit$finalModel
fm
```
Note the OOB (out-of-bag) error estimate rate of 1.3% which is the random forest out-of-sample error estimate.   

The top 5 most important variables are listed below along with a plot of the top two variables color coded by the classe outcomes.  Note how there is structure to the data, but the decision tree rules themselves are not obvious.

```{r}
vi <- varImp(fm)
vi <- vi[order(vi,decreasing=TRUE),1,drop=FALSE] # sort decreasing and keep row names
vi[1:5,1,drop=FALSE] # top five important variables for model (drop=F => with names)
# plot top two variables color coded by outcome
qplot(magnet_dumbbell_z, magnet_dumbbell_y, colour=classe, data=training4)
```

### Cross-Validation out-of-sample error

Predictions are made for the cross validation test dataset and are compared to the actual values.
```{r}
# make cross validation prediction on test data
predTest <- predict(modFit, newdata=testing)
# tabulate prediction versus actual in cross validation test dataset
tbl <- table(predTest,testing$classe)
# determine cross validation out of sample error (oose)
oose <- 1- sum(table(predTest,testing$classe)*diag(rep(1,5)))/
    sum(table(predTest,testing$classe))
oose
```
Note that this cross-validation out-of-sample-error of 1.1% is close to the random forest OOB estimate.

### Conclusions
This weight lifting activity quality dataset and the random forest model indicate that these types of sensors could be used to provide real time feedback to an individual on the quality of their weight lifting exercises.
